{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775fdd95",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-19T06:08:05.513839Z",
     "iopub.status.busy": "2025-11-19T06:08:05.513535Z",
     "iopub.status.idle": "2025-11-19T06:13:06.077841Z",
     "shell.execute_reply": "2025-11-19T06:13:06.076618Z"
    },
    "papermill": {
     "duration": 300.570578,
     "end_time": "2025-11-19T06:13:06.079541",
     "exception": false,
     "start_time": "2025-11-19T06:08:05.508963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT: COVERTYPE Dataset (FULL DATA)\n",
      "================================================================================\n",
      "[INFO] Fetching Covertype (sklearn.fetch_covtype)...\n",
      "   Total Samples: 11620\n",
      "   Features: 54\n",
      "   Classes: 7\n",
      "   Using ALL 11620 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 401/401 [01:06<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 0.9897\n",
      "Random Forest        0.9661\n",
      "SVM                  0.7550\n",
      "CART                 0.9801\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 0.9928 (4271/4302)\n",
      "  Class 1: 0.9852 (5516/5599)\n",
      "  Class 2: 1.0000 (728/728)\n",
      "  Class 3: 1.0000 (52/52)\n",
      "  Class 4: 1.0000 (193/193)\n",
      "  Class 5: 0.9828 (343/349)\n",
      "  Class 6: 1.0000 (397/397)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: POKER Dataset (FULL DATA)\n",
      "================================================================================\n",
      "   Total Samples: 250\n",
      "   Features: 10\n",
      "   Classes: 7\n",
      "   Using ALL 250 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data:  50%|████▉     | 200/401 [00:18<00:18, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 1.0000\n",
      "Random Forest        1.0000\n",
      "SVM                  0.7200\n",
      "CART                 1.0000\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 1.0000 (126/126)\n",
      "  Class 1: 1.0000 (101/101)\n",
      "  Class 2: 1.0000 (13/13)\n",
      "  Class 3: 1.0000 (5/5)\n",
      "  Class 4: 1.0000 (3/3)\n",
      "  Class 5: 1.0000 (1/1)\n",
      "  Class 6: 1.0000 (1/1)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: LETTER Dataset (FULL DATA)\n",
      "================================================================================\n",
      "   Total Samples: 20000\n",
      "   Features: 16\n",
      "   Classes: 26\n",
      "   Using ALL 20000 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 801/801 [00:44<00:00, 17.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 0.9851\n",
      "Random Forest        0.9998\n",
      "SVM                  0.9627\n",
      "CART                 0.9861\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 1.0000 (789/789)\n",
      "  Class 1: 0.9569 (733/766)\n",
      "  Class 2: 0.9932 (731/736)\n",
      "  Class 3: 0.9789 (788/805)\n",
      "  Class 4: 0.9792 (752/768)\n",
      "  Class 5: 0.9897 (767/775)\n",
      "  Class 6: 0.9780 (756/773)\n",
      "  Class 7: 0.9700 (712/734)\n",
      "  Class 8: 0.9669 (730/755)\n",
      "  Class 9: 0.9772 (730/747)\n",
      "  Class 10: 0.9878 (730/739)\n",
      "  Class 11: 0.9895 (753/761)\n",
      "  Class 12: 0.9937 (787/792)\n",
      "  Class 13: 0.9860 (772/783)\n",
      "  Class 14: 0.9867 (743/753)\n",
      "  Class 15: 0.9714 (780/803)\n",
      "  Class 16: 1.0000 (783/783)\n",
      "  Class 17: 0.9644 (731/758)\n",
      "  Class 18: 0.9973 (746/748)\n",
      "  Class 19: 0.9937 (791/796)\n",
      "  Class 20: 0.9938 (808/813)\n",
      "  Class 21: 0.9751 (745/764)\n",
      "  Class 22: 0.9973 (750/752)\n",
      "  Class 23: 0.9975 (785/787)\n",
      "  Class 24: 0.9898 (778/786)\n",
      "  Class 25: 0.9986 (733/734)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: PENDIGITS Dataset (FULL DATA)\n",
      "================================================================================\n",
      "   Total Samples: 7494\n",
      "   Features: 16\n",
      "   Classes: 10\n",
      "   Using ALL 7494 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 801/801 [00:26<00:00, 30.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 1.0000\n",
      "Random Forest        1.0000\n",
      "SVM                  0.9976\n",
      "CART                 1.0000\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 1.0000 (780/780)\n",
      "  Class 1: 1.0000 (779/779)\n",
      "  Class 2: 1.0000 (780/780)\n",
      "  Class 3: 1.0000 (719/719)\n",
      "  Class 4: 1.0000 (780/780)\n",
      "  Class 5: 1.0000 (720/720)\n",
      "  Class 6: 1.0000 (720/720)\n",
      "  Class 7: 1.0000 (778/778)\n",
      "  Class 8: 1.0000 (719/719)\n",
      "  Class 9: 1.0000 (719/719)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: GAS Dataset (FULL DATA)\n",
      "================================================================================\n",
      "[ERROR] Could not load dataset gas: Error loading Gas dataset: Failed to read CSV from https://archive.ics.uci.edu/ml/machine-learning-databases/00336/GasSensors.csv: HTTP Error 404: Not Found\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: SATIMAGE Dataset (FULL DATA)\n",
      "================================================================================\n",
      "   Total Samples: 6435\n",
      "   Features: 36\n",
      "   Classes: 6\n",
      "   Using ALL 6435 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data:   0%|          | 0/801 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Experiment satimage failed: Target 6 is out of bounds.\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: IRIS Dataset (FULL DATA)\n",
      "================================================================================\n",
      "   Total Samples: 150\n",
      "   Features: 4\n",
      "   Classes: 3\n",
      "   Using ALL 150 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 501/501 [00:04<00:00, 102.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 0.9867\n",
      "Random Forest        1.0000\n",
      "SVM                  0.9733\n",
      "CART                 1.0000\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 1.0000 (50/50)\n",
      "  Class 1: 0.9800 (49/50)\n",
      "  Class 2: 0.9800 (49/50)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF ALL EXPERIMENTS (FULL DATASET RESULTS)\n",
      "================================================================================\n",
      "\n",
      "Dataset         Samples    Features   Classes    DLGN       RF         SVM        CART      \n",
      "-----------------------------------------------------------------------------------------------\n",
      "covertype       11620      54         7          0.9897     0.9661     0.7550     0.9801\n",
      "poker           250        10         7          1.0000     1.0000     0.7200     1.0000\n",
      "letter          20000      16         26         0.9851     0.9998     0.9627     0.9861\n",
      "pendigits       7494       16         10         1.0000     1.0000     0.9976     1.0000\n",
      "iris            150        4          3          0.9867     1.0000     0.9733     1.0000\n",
      "\n",
      "ALL EXPERIMENTS COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# full_recode_7_datasets_no_higgs.py\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_iris, load_wine, fetch_covtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =======================================================================\n",
    "# UTIL\n",
    "# =======================================================================\n",
    "\n",
    "def set_npseed(seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def set_torchseed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def safe_read_csv(url, **kwargs):\n",
    "    try:\n",
    "        return pd.read_csv(url, **kwargs)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read CSV from {url}: {e}\")\n",
    "\n",
    "# =======================================================================\n",
    "# DATA LOADERS (7 datasets). HIGGS removed.\n",
    "# - covertype, poker (fixed), letter, pendigits, gas, satimage, plus iris/wine\n",
    "# =======================================================================\n",
    "\n",
    "def load_dataset(dataset_name, sample_frac=0.05, random_state=42):\n",
    "    dataset_name = dataset_name.lower()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "        X, y = data.data, data.target\n",
    "        feature_names = data.feature_names\n",
    "\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "        X, y = data.data, data.target\n",
    "        feature_names = data.feature_names\n",
    "\n",
    "    elif dataset_name == 'covertype':\n",
    "        print(\"[INFO] Fetching Covertype (sklearn.fetch_covtype)...\")\n",
    "        cov = fetch_covtype()\n",
    "        X = cov.data\n",
    "        y = cov.target - 1\n",
    "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        if sample_frac < 1.0:\n",
    "            idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "            X, y = X[idx], y[idx]\n",
    "\n",
    "    elif dataset_name == 'poker':\n",
    "        # Robust Poker loader: try direct file(s) then fallback to zip archive\n",
    "        # UCI sometimes exposes training/testing files inside a zip; we'll handle both.\n",
    "        tried = []\n",
    "        df = None\n",
    "        candidates = [\n",
    "            \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand.data\",\n",
    "            \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-training-true.data\",\n",
    "            \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand-testing.data\"\n",
    "        ]\n",
    "        for url in candidates:\n",
    "            try:\n",
    "                tried.append(url)\n",
    "                df = safe_read_csv(url, header=None)\n",
    "                break\n",
    "            except Exception:\n",
    "                df = None\n",
    "        if df is None:\n",
    "            # fallback: download zip and extract training/testing files (stable path)\n",
    "            zip_url = \"https://archive.ics.uci.edu/static/public/158/poker+hand.zip\"\n",
    "            try:\n",
    "                print(f\"[INFO] Direct files not found; downloading zip {zip_url} ...\")\n",
    "                resp = urllib.request.urlopen(zip_url)\n",
    "                zbuf = io.BytesIO(resp.read())\n",
    "                with zipfile.ZipFile(zbuf) as zf:\n",
    "                    # find likely files inside zip\n",
    "                    names = zf.namelist()\n",
    "                    # prefer combined file or training+testing\n",
    "                    candidate_names = [n for n in names if n.lower().endswith('.data') or n.lower().endswith('.csv')]\n",
    "                    # read and concatenate any matching files\n",
    "                    parts = []\n",
    "                    for n in candidate_names:\n",
    "                        with zf.open(n) as fh:\n",
    "                            part = pd.read_csv(fh, header=None)\n",
    "                            parts.append(part)\n",
    "                    if len(parts) == 0:\n",
    "                        raise RuntimeError(\"No .data/.csv parts found inside poker zip.\")\n",
    "                    df = pd.concat(parts, ignore_index=True)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error loading Poker dataset (tried {tried} and zip): {e}\")\n",
    "\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        if sample_frac < 1.0:\n",
    "            idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "            X, y = X[idx], y[idx]\n",
    "\n",
    "    elif dataset_name == 'letter':\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\"\n",
    "        df = safe_read_csv(url, header=None)\n",
    "        labels = df.iloc[:, 0].values\n",
    "        unique = np.unique(labels)\n",
    "        label_map = {ch: i for i, ch in enumerate(unique)}\n",
    "        y = np.array([label_map[ch] for ch in labels])\n",
    "        X = df.iloc[:, 1:].values\n",
    "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        if sample_frac < 1.0:\n",
    "            idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "            X, y = X[idx], y[idx]\n",
    "\n",
    "    elif dataset_name in ('pendigits', 'pen'):\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\"\n",
    "        df = safe_read_csv(url, header=None)\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        if sample_frac < 1.0:\n",
    "            idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "            X, y = X[idx], y[idx]\n",
    "\n",
    "    elif dataset_name == 'gas':\n",
    "        # Attempting a UCI-hosted CSV (formats vary); try the most likely path.\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00336/GasSensors.csv\"\n",
    "        try:\n",
    "            df = safe_read_csv(url, header=0)\n",
    "            X = df.iloc[:, :-1].values\n",
    "            y_raw = df.iloc[:, -1].values\n",
    "            if not np.issubdtype(y_raw.dtype, np.integer):\n",
    "                uniq = np.unique(y_raw)\n",
    "                m = {v: i for i, v in enumerate(uniq)}\n",
    "                y = np.array([m[v] for v in y_raw])\n",
    "            else:\n",
    "                y = y_raw\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            if sample_frac < 1.0:\n",
    "                idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "                X, y = X[idx], y[idx]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading Gas dataset: {e}\")\n",
    "\n",
    "    elif dataset_name == 'satimage':\n",
    "        train_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\"\n",
    "        test_url  = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.tst\"\n",
    "        df_train = safe_read_csv(train_url, header=None, sep=r'\\s+')\n",
    "        df_test  = safe_read_csv(test_url, header=None, sep=r'\\s+')\n",
    "        df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values - 1\n",
    "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    return X, y, feature_names\n",
    "\n",
    "# =======================================================================\n",
    "# DLGN MODEL (cleaned)\n",
    "# =======================================================================\n",
    "\n",
    "class DLGN_FC(nn.Module):\n",
    "    def __init__(self, input_dim=None, output_dim=None, num_hidden_nodes=None, \n",
    "                 beta=30, dlgn_mode='dlgn_sf', mode='pwc', num_classes=4):\n",
    "        super(DLGN_FC, self).__init__()\n",
    "        if num_hidden_nodes is None:\n",
    "            num_hidden_nodes = []\n",
    "        self.num_hidden_layers = len(num_hidden_nodes)\n",
    "        self.beta = beta\n",
    "        self.dlgn_mode = dlgn_mode\n",
    "        self.mode = mode\n",
    "        self.num_classes = num_classes\n",
    "        self.num_nodes = [input_dim] + list(num_hidden_nodes) + [num_classes]\n",
    "        self.gating_layers = nn.ModuleList()\n",
    "        self.value_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_hidden_layers+1):\n",
    "            if i != self.num_hidden_layers:\n",
    "                if self.dlgn_mode == 'dlgn_sf':\n",
    "                    temp = nn.Linear(self.num_nodes[0], self.num_nodes[i+1], bias=False)\n",
    "                else:\n",
    "                    temp = nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False)\n",
    "                self.gating_layers.append(temp)\n",
    "            temp = nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False)\n",
    "            self.value_layers.append(temp)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        device = x.device if x.is_cuda else torch.device('cpu')\n",
    "        gate_scores = [x.to(device)]\n",
    "        if self.mode == 'pwc':\n",
    "            values = [torch.ones_like(x).to(device)]\n",
    "        else:\n",
    "            values = [x.to(device)]\n",
    "\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            if self.dlgn_mode == 'dlgn_sf':\n",
    "                gate_scores.append((x.to(device) @ self.gating_layers[i].weight.T))\n",
    "            else:\n",
    "                gate_scores.append(self.gating_layers[i](gate_scores[-1]))\n",
    "            curr_gate_on_off = torch.sigmoid(self.beta * gate_scores[-1])\n",
    "            values.append(self.value_layers[i](values[-1]) * curr_gate_on_off)\n",
    "        \n",
    "        values.append(self.value_layers[self.num_hidden_layers](values[-1]))\n",
    "        return values, gate_scores\n",
    "\n",
    "# =======================================================================\n",
    "# TRAIN / BASELINES / RUN FUNCTIONS (kept similar to yours)\n",
    "# =======================================================================\n",
    "\n",
    "def train_dlgn_full(DLGN_obj, data_full, labels_full, \n",
    "                    num_epoch=1500, parameter_mask=None, seed=42, lr=0.001, \n",
    "                    no_of_batches=10, saved_epochs=None, x_epoch=1000):\n",
    "    if parameter_mask is None:\n",
    "        parameter_mask = {name: torch.ones_like(param) for name, param in DLGN_obj.named_parameters()}\n",
    "    if saved_epochs is None:\n",
    "        saved_epochs = list(range(0, num_epoch, 100)) + [num_epoch-1]\n",
    "\n",
    "    set_torchseed(seed)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    DLGN_obj.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(DLGN_obj.parameters(), lr=lr)\n",
    "\n",
    "    data_torch = torch.Tensor(data_full)\n",
    "    labels_torch = torch.tensor(labels_full, dtype=torch.int64)\n",
    "\n",
    "    batch_size = max(1, len(data_full) // max(1, no_of_batches))\n",
    "    losses = []\n",
    "    DLGN_obj_store = []\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(saved_epochs[-1] + 1), desc=\"Training on Full Data\"):\n",
    "        if epoch in saved_epochs:\n",
    "            DLGN_obj_copy = deepcopy(DLGN_obj)\n",
    "            DLGN_obj_copy.to(torch.device('cpu'))\n",
    "            DLGN_obj_store.append(DLGN_obj_copy)\n",
    "\n",
    "            train_outputs_values, _ = DLGN_obj(torch.Tensor(data_full).to(device))\n",
    "            targets = torch.tensor(labels_full, dtype=torch.int64).to(device)\n",
    "            train_loss = criterion(train_outputs_values[-1], targets)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            if train_loss.item() < 5e-6 or np.isnan(train_loss.detach().cpu().numpy()):\n",
    "                break\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for batch_start in range(0, len(data_full), batch_size):\n",
    "            if (batch_start + batch_size) > len(data_full):\n",
    "                inputs = data_torch[batch_start:len(data_full)]\n",
    "                targets = labels_torch[batch_start:len(data_full)]\n",
    "            else:\n",
    "                inputs = data_torch[batch_start:batch_start+batch_size]\n",
    "                targets = labels_torch[batch_start:batch_start+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            values, _ = DLGN_obj(inputs)\n",
    "            outputs = values[-1]\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            for name, param in DLGN_obj.named_parameters():\n",
    "                parameter_mask[name] = parameter_mask[name].to(device)\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= parameter_mask[name]\n",
    "                    if \"gat\" in name and epoch > x_epoch:\n",
    "                        param.grad *= 0.\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_outputs_values, _ = DLGN_obj(torch.Tensor(data_full).to(device))\n",
    "        targets = torch.tensor(labels_full, dtype=torch.int64).to(device)\n",
    "        train_loss = criterion(train_outputs_values[-1], targets)\n",
    "        losses.append(train_loss.cpu().detach().clone().numpy())\n",
    "\n",
    "    DLGN_obj.to(torch.device('cpu'))\n",
    "    return train_losses, DLGN_obj, DLGN_obj_store, losses\n",
    "\n",
    "def train_baselines_full(X_full, y_full):\n",
    "    results = {}\n",
    "    print(\"  Training Random Forest on full data...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=20)\n",
    "    rf.fit(X_full, y_full)\n",
    "    results['Random Forest'] = accuracy_score(y_full, rf.predict(X_full))\n",
    "\n",
    "    print(\"  Training SVM on full data (may be slow for large datasets)...\")\n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_full, y_full)\n",
    "    results['SVM'] = accuracy_score(y_full, svm.predict(X_full))\n",
    "\n",
    "    print(\"  Training CART on full data...\")\n",
    "    cart = DecisionTreeClassifier(random_state=42, max_depth=20)\n",
    "    cart.fit(X_full, y_full)\n",
    "    results['CART'] = accuracy_score(y_full, cart.predict(X_full))\n",
    "    return results\n",
    "\n",
    "def run_experiment_full(dataset_name, config, sample_frac=None):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXPERIMENT: {dataset_name.upper()} Dataset (FULL DATA)\")\n",
    "    print(\"=\"*80)\n",
    "    try:\n",
    "        if sample_frac is None:\n",
    "            X, y, feature_names = load_dataset(dataset_name)\n",
    "        else:\n",
    "            X, y, feature_names = load_dataset(dataset_name, sample_frac=sample_frac)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not load dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    num_classes = len(np.unique(y))\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    print(f\"   Total Samples: {X.shape[0]}\")\n",
    "    print(f\"   Features: {input_dim}\")\n",
    "    print(f\"   Classes: {num_classes}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    print(f\"   Using ALL {len(X_scaled)} samples for training AND testing\")\n",
    "\n",
    "    print(\"\\n2. Training Baseline Models on Full Data...\")\n",
    "    baseline_results = train_baselines_full(X_scaled, y)\n",
    "\n",
    "    print(\"\\n3. Training DLGN on Full Data...\")\n",
    "    set_torchseed(config.get('seed', 6675))\n",
    "    DLGN_model = DLGN_FC(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=1,\n",
    "        num_hidden_nodes=config['hidden_nodes'],\n",
    "        beta=config['beta'],\n",
    "        dlgn_mode=config.get('dlgn_mode', 'dlgn'),\n",
    "        mode='pwc',\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    train_parameter_masks = {name: torch.ones_like(param) for name, param in DLGN_model.named_parameters()}\n",
    "\n",
    "    train_losses, DLGN_final, DLGN_store, losses = train_dlgn_full(\n",
    "        DLGN_obj=deepcopy(DLGN_model),\n",
    "        data_full=X_scaled,\n",
    "        labels_full=y,\n",
    "        parameter_mask=train_parameter_masks,\n",
    "        lr=config['lr'],\n",
    "        no_of_batches=config['batches'],\n",
    "        saved_epochs=config['saved_epochs'],\n",
    "        x_epoch=config['x_epoch'],\n",
    "        num_epoch=config['num_epochs'],\n",
    "        seed=config.get('seed', 5000)\n",
    "    )\n",
    "\n",
    "    full_outputs_values, _ = DLGN_final(torch.Tensor(X_scaled))\n",
    "    full_logits = full_outputs_values[-1].detach().numpy()\n",
    "    full_preds = np.argmax(full_logits, axis=1)\n",
    "    dlgn_acc = accuracy_score(y, full_preds)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS (Training and Testing on FULL DATASET)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Model':<20} {'Accuracy on Full Data':<25}\")\n",
    "    print(\"-\"*45)\n",
    "    print(f\"{'DLGN':<20} {dlgn_acc:.4f}\")\n",
    "    for model_name, acc in baseline_results.items():\n",
    "        print(f\"{model_name:<20} {acc:.4f}\")\n",
    "\n",
    "    print(f\"\\nDLGN Per-Class Accuracy (on full data):\")\n",
    "    for i in range(num_classes):\n",
    "        class_mask = y == i\n",
    "        class_correct = np.sum((y[class_mask] == full_preds[class_mask]))\n",
    "        class_total = np.sum(class_mask)\n",
    "        if class_total > 0:\n",
    "            print(f\"  Class {i}: {class_correct / class_total:.4f} ({class_correct}/{class_total})\")\n",
    "\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'dlgn_acc': dlgn_acc,\n",
    "        'baseline_results': baseline_results,\n",
    "        'num_samples': X.shape[0],\n",
    "        'num_features': input_dim,\n",
    "        'num_classes': num_classes\n",
    "    }\n",
    "\n",
    "# =======================================================================\n",
    "# MAIN: datasets list (HIGGS removed)\n",
    "# =======================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    small_dataset_config = {\n",
    "        'hidden_nodes': [10, 10, 10],\n",
    "        'beta': 4,\n",
    "        'lr': 0.002,\n",
    "        'batches': 5,\n",
    "        'dlgn_mode': 'dlgn',\n",
    "        'num_epochs': 500,\n",
    "        'saved_epochs': list(range(0, 501, 100)),\n",
    "        'x_epoch': 300,\n",
    "        'seed': 6675\n",
    "    }\n",
    "\n",
    "    medium_dataset_config = {\n",
    "        'hidden_nodes': [20, 20, 20],\n",
    "        'beta': 4,\n",
    "        'lr': 0.002,\n",
    "        'batches': 10,\n",
    "        'dlgn_mode': 'dlgn',\n",
    "        'num_epochs': 800,\n",
    "        'saved_epochs': list(range(0, 801, 100)),\n",
    "        'x_epoch': 500,\n",
    "        'seed': 6675\n",
    "    }\n",
    "\n",
    "    large_dataset_config = {\n",
    "        'hidden_nodes': [64, 64, 32],\n",
    "        'beta': 4,\n",
    "        'lr': 0.001,\n",
    "        'batches': 50,\n",
    "        'dlgn_mode': 'dlgn',\n",
    "        'num_epochs': 400,\n",
    "        'saved_epochs': list(range(0, 401, 50)),\n",
    "        'x_epoch': 300,\n",
    "        'seed': 6675\n",
    "    }\n",
    "\n",
    "    # Updated datasets (HIGGS removed)\n",
    "    datasets_and_configs = [\n",
    "        ('covertype', large_dataset_config, 0.02),  # sample small %\n",
    "        ('poker', large_dataset_config, 0.01),      # poker: fixed loader (uses zip fallback)\n",
    "        ('letter', medium_dataset_config, 1.0),\n",
    "        ('pendigits', medium_dataset_config, 1.0),\n",
    "        ('gas', medium_dataset_config, 1.0),\n",
    "        ('satimage', medium_dataset_config, 1.0),\n",
    "        ('iris', small_dataset_config, 1.0)         # keep an easy one for sanity-check\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    for name, cfg, sample_frac in datasets_and_configs:\n",
    "        try:\n",
    "            res = run_experiment_full(name, cfg, sample_frac=sample_frac)\n",
    "            if res:\n",
    "                all_results.append(res)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Experiment {name} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    if all_results:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF ALL EXPERIMENTS (FULL DATASET RESULTS)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\n{'Dataset':<15} {'Samples':<10} {'Features':<10} {'Classes':<10} {'DLGN':<10} {'RF':<10} {'SVM':<10} {'CART':<10}\")\n",
    "        print(\"-\"*95)\n",
    "        for r in all_results:\n",
    "            ds = r['dataset']\n",
    "            print(f\"{ds:<15} {r['num_samples']:<10} {r['num_features']:<10} {r['num_classes']:<10} \"\n",
    "                  f\"{r['dlgn_acc']:.4f}     {r['baseline_results']['Random Forest']:.4f}     \"\n",
    "                  f\"{r['baseline_results']['SVM']:.4f}     {r['baseline_results']['CART']:.4f}\")\n",
    "        print(\"\\nALL EXPERIMENTS COMPLETE!\")\n",
    "    else:\n",
    "        print(\"No successful experiments completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99211c02",
   "metadata": {
    "papermill": {
     "duration": 0.053909,
     "end_time": "2025-11-19T06:13:06.188386",
     "exception": false,
     "start_time": "2025-11-19T06:13:06.134477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 308.738225,
   "end_time": "2025-11-19T06:13:08.726449",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T06:07:59.988224",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
