{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14caeb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T06:07:47.523564Z",
     "iopub.status.busy": "2025-11-19T06:07:47.523278Z",
     "iopub.status.idle": "2025-11-19T06:17:24.500044Z",
     "shell.execute_reply": "2025-11-19T06:17:24.498844Z"
    },
    "papermill": {
     "duration": 576.982361,
     "end_time": "2025-11-19T06:17:24.501507",
     "exception": false,
     "start_time": "2025-11-19T06:07:47.519146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT: COVERTYPE Dataset (FULL DATA)\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "[INFO] Fetching Covertype (may take a bit)...\n",
      "   Total Samples: 11620\n",
      "   Features: 54\n",
      "   Classes: 7\n",
      "   Using ALL 11620 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 401/401 [01:02<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 0.9897\n",
      "Random Forest        0.9661\n",
      "SVM                  0.7550\n",
      "CART                 0.9801\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 0.9928 (4271/4302)\n",
      "  Class 1: 0.9852 (5516/5599)\n",
      "  Class 2: 1.0000 (728/728)\n",
      "  Class 3: 1.0000 (52/52)\n",
      "  Class 4: 1.0000 (193/193)\n",
      "  Class 5: 0.9828 (343/349)\n",
      "  Class 6: 1.0000 (397/397)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: POKER Dataset (FULL DATA)\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "[INFO] Downloading Poker dataset (UCI).\n",
      "[ERROR] Could not load dataset poker: Error loading Poker dataset: Failed to read CSV from https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand.data: HTTP Error 404: Not Found\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: HIGGS Dataset (FULL DATA)\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "[INFO] Loading HIGGS (streaming, sampled). This may take time if sample_frac ~1.0\n",
      "   Total Samples: 11000\n",
      "   Features: 28\n",
      "   Classes: 2\n",
      "   Using ALL 11000 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 401/401 [00:57<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 1.0000\n",
      "Random Forest        0.9999\n",
      "SVM                  0.7616\n",
      "CART                 0.9885\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 1.0000 (5112/5112)\n",
      "  Class 1: 1.0000 (5888/5888)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: LETTER Dataset (FULL DATA)\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "   Total Samples: 20000\n",
      "   Features: 16\n",
      "   Classes: 26\n",
      "   Using ALL 20000 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 801/801 [00:42<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 0.9851\n",
      "Random Forest        0.9998\n",
      "SVM                  0.9627\n",
      "CART                 0.9861\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 1.0000 (789/789)\n",
      "  Class 1: 0.9569 (733/766)\n",
      "  Class 2: 0.9932 (731/736)\n",
      "  Class 3: 0.9789 (788/805)\n",
      "  Class 4: 0.9792 (752/768)\n",
      "  Class 5: 0.9897 (767/775)\n",
      "  Class 6: 0.9780 (756/773)\n",
      "  Class 7: 0.9700 (712/734)\n",
      "  Class 8: 0.9669 (730/755)\n",
      "  Class 9: 0.9772 (730/747)\n",
      "  Class 10: 0.9878 (730/739)\n",
      "  Class 11: 0.9895 (753/761)\n",
      "  Class 12: 0.9937 (787/792)\n",
      "  Class 13: 0.9860 (772/783)\n",
      "  Class 14: 0.9867 (743/753)\n",
      "  Class 15: 0.9714 (780/803)\n",
      "  Class 16: 1.0000 (783/783)\n",
      "  Class 17: 0.9644 (731/758)\n",
      "  Class 18: 0.9973 (746/748)\n",
      "  Class 19: 0.9937 (791/796)\n",
      "  Class 20: 0.9938 (808/813)\n",
      "  Class 21: 0.9751 (745/764)\n",
      "  Class 22: 0.9973 (750/752)\n",
      "  Class 23: 0.9975 (785/787)\n",
      "  Class 24: 0.9898 (778/786)\n",
      "  Class 25: 0.9986 (733/734)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: PENDIGITS Dataset (FULL DATA)\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "   Total Samples: 7494\n",
      "   Features: 16\n",
      "   Classes: 10\n",
      "   Using ALL 7494 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data: 100%|██████████| 801/801 [00:24<00:00, 32.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS (Training and Testing on FULL DATASET)\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy on Full Data    \n",
      "---------------------------------------------\n",
      "DLGN                 1.0000\n",
      "Random Forest        1.0000\n",
      "SVM                  0.9976\n",
      "CART                 1.0000\n",
      "\n",
      "DLGN Per-Class Accuracy (on full data):\n",
      "  Class 0: 1.0000 (780/780)\n",
      "  Class 1: 1.0000 (779/779)\n",
      "  Class 2: 1.0000 (780/780)\n",
      "  Class 3: 1.0000 (719/719)\n",
      "  Class 4: 1.0000 (780/780)\n",
      "  Class 5: 1.0000 (720/720)\n",
      "  Class 6: 1.0000 (720/720)\n",
      "  Class 7: 1.0000 (778/778)\n",
      "  Class 8: 1.0000 (719/719)\n",
      "  Class 9: 1.0000 (719/719)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: GAS Dataset (FULL DATA)\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "[ERROR] Could not load dataset gas: Error loading Gas dataset: Failed to read CSV from https://archive.ics.uci.edu/ml/machine-learning-databases/00336/GasSensors.csv: HTTP Error 404: Not Found\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT: SATIMAGE Dataset (FULL DATA)\n",
      "================================================================================\n",
      "\n",
      "1. Loading data...\n",
      "   Total Samples: 6435\n",
      "   Features: 36\n",
      "   Classes: 6\n",
      "   Using ALL 6435 samples for training AND testing\n",
      "\n",
      "2. Training Baseline Models on Full Data...\n",
      "  Training Random Forest on full data...\n",
      "  Training SVM on full data (may be slow for large datasets)...\n",
      "  Training CART on full data...\n",
      "\n",
      "3. Training DLGN on Full Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on Full Data:   0%|          | 0/801 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Experiment satimage failed: Target 6 is out of bounds.\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF ALL EXPERIMENTS (FULL DATASET RESULTS)\n",
      "================================================================================\n",
      "\n",
      "Dataset         Samples    Features   Classes    DLGN       RF         SVM        CART      \n",
      "-----------------------------------------------------------------------------------------------\n",
      "covertype       11620      54         7          0.9897     0.9661     0.7550     0.9801\n",
      "higgs           11000      28         2          1.0000     0.9999     0.7616     0.9885\n",
      "letter          20000      16         26         0.9851     0.9998     0.9627     0.9861\n",
      "pendigits       7494       16         10         1.0000     1.0000     0.9976     1.0000\n",
      "\n",
      "ALL EXPERIMENTS COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# full_recode_with_7_datasets.py\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import load_iris, load_wine, fetch_covtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =======================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =======================================================================\n",
    "\n",
    "def set_npseed(seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def set_torchseed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def safe_read_csv(url, **kwargs):\n",
    "    \"\"\"Read CSV with a helpful error message.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(url, **kwargs)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read CSV from {url}: {e}\")\n",
    "\n",
    "# =======================================================================\n",
    "# DATA LOADING: 7 datasets\n",
    "# - covertype (sklearn fetch)\n",
    "# - poker (UCI)\n",
    "# - higgs (UCI, huge -> sampled)\n",
    "# - letter (UCI)\n",
    "# - pendigits (UCI)\n",
    "# - gas (UCI sensor drift)\n",
    "# - satimage (UCI)\n",
    "# Each loader returns (X, y, feature_names)\n",
    "# =======================================================================\n",
    "\n",
    "def load_dataset(dataset_name, sample_frac=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Load a dataset by name.\n",
    "    - sample_frac: fraction to sample for very large datasets (0 < sample_frac <= 1.0)\n",
    "    \"\"\"\n",
    "    dataset_name = dataset_name.lower()\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    if dataset_name == 'iris':\n",
    "        data = load_iris()\n",
    "        X, y = data.data, data.target\n",
    "        feature_names = data.feature_names\n",
    "\n",
    "    elif dataset_name == 'wine':\n",
    "        data = load_wine()\n",
    "        X, y = data.data, data.target\n",
    "        feature_names = data.feature_names\n",
    "\n",
    "    elif dataset_name == 'covertype':\n",
    "        # Large but manageable with sklearn\n",
    "        try:\n",
    "            print(\"[INFO] Fetching Covertype (may take a bit)...\")\n",
    "            cov = fetch_covtype()\n",
    "            X = cov.data\n",
    "            y = cov.target - 1  # make 0-indexed\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            if sample_frac < 1.0:\n",
    "                idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "                X = X[idx]\n",
    "                y = y[idx]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error fetching covertype: {e}\")\n",
    "\n",
    "    elif dataset_name == 'poker':\n",
    "        # Poker-hand (UCI) - very large ~1M rows\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/poker/poker-hand.data\"\n",
    "        try:\n",
    "            print(\"[INFO] Downloading Poker dataset (UCI).\")\n",
    "            df = safe_read_csv(url, header=None)\n",
    "            X = df.iloc[:, :-1].values\n",
    "            y = df.iloc[:, -1].values\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            # sample if needed\n",
    "            if sample_frac < 1.0:\n",
    "                idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "                X = X[idx]; y = y[idx]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading Poker dataset: {e}\")\n",
    "\n",
    "    elif dataset_name == 'higgs':\n",
    "        # HIGGS dataset (very large). default: sample small fraction\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"\n",
    "        try:\n",
    "            print(\"[INFO] Loading HIGGS (streaming, sampled). This may take time if sample_frac ~1.0\")\n",
    "            # read in chunks and sample rows to avoid loading full file into memory\n",
    "            chunks = []\n",
    "            chunk_iter = pd.read_csv(url, header=None, compression='gzip', chunksize=200000)\n",
    "            for chunk in chunk_iter:\n",
    "                if sample_frac >= 1.0:\n",
    "                    chunks.append(chunk)\n",
    "                else:\n",
    "                    # sample rows from chunk\n",
    "                    s = chunk.sample(frac=sample_frac, random_state=random_state)\n",
    "                    chunks.append(s)\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            # First column is label in original HIGGS (0/1); rest are features\n",
    "            y = df.iloc[:, 0].values\n",
    "            X = df.iloc[:, 1:].values\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            # convert to multiclass placeholder if you want multi-class - here keep binary as-is\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading HIGGS dataset: {e}\")\n",
    "\n",
    "    elif dataset_name == 'letter':\n",
    "        # Letter Recognition\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\"\n",
    "        try:\n",
    "            df = safe_read_csv(url, header=None)\n",
    "            # first column is letter A-Z\n",
    "            labels = df.iloc[:, 0].values\n",
    "            unique = np.unique(labels)\n",
    "            label_map = {ch: i for i, ch in enumerate(unique)}\n",
    "            y = np.array([label_map[ch] for ch in labels])\n",
    "            X = df.iloc[:, 1:].values\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            if sample_frac < 1.0:\n",
    "                idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "                X = X[idx]; y = y[idx]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading Letter dataset: {e}\")\n",
    "\n",
    "    elif dataset_name == 'pendigits' or dataset_name == 'pen':\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra\"\n",
    "        try:\n",
    "            df = safe_read_csv(url, header=None)\n",
    "            X = df.iloc[:, :-1].values\n",
    "            y = df.iloc[:, -1].values\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            if sample_frac < 1.0:\n",
    "                idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "                X = X[idx]; y = y[idx]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading PenDigits dataset: {e}\")\n",
    "\n",
    "    elif dataset_name == 'gas':\n",
    "        # Gas Sensor Array Drift dataset (a cleaned / packaged CSV location may vary)\n",
    "        # Using the UCI page: actual download is more complex; try a packaged CSV if available\n",
    "        # We'll try an archived CSV (if reachable) else raise informative error\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00336/GasSensors.csv\"\n",
    "        try:\n",
    "            df = safe_read_csv(url, header=0)\n",
    "            # This dataset formats vary - try to detect last column as label if integer-like\n",
    "            if df.shape[1] < 2:\n",
    "                raise RuntimeError(\"Unexpected gas dataset format.\")\n",
    "            X = df.iloc[:, :-1].values\n",
    "            y_raw = df.iloc[:, -1].values\n",
    "            # If labels are strings, map to ints\n",
    "            unique = np.unique(y_raw)\n",
    "            if not np.issubdtype(y_raw.dtype, np.integer):\n",
    "                map_ = {v: i for i, v in enumerate(unique)}\n",
    "                y = np.array([map_[v] for v in y_raw])\n",
    "            else:\n",
    "                y = y_raw\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            if sample_frac < 1.0:\n",
    "                idx = rng.choice(len(X), size=int(len(X)*sample_frac), replace=False)\n",
    "                X = X[idx]; y = y[idx]\n",
    "        except Exception as e:\n",
    "            # As a safer fallback: try the \"gas sensor array drift dataset\" page files\n",
    "            raise RuntimeError(f\"Error loading Gas dataset: {e}\")\n",
    "\n",
    "    elif dataset_name == 'satimage':\n",
    "        try:\n",
    "            train_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn\"\n",
    "            test_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.tst\"\n",
    "            df_train = safe_read_csv(train_url, header=None, sep=r'\\s+')\n",
    "            df_test = safe_read_csv(test_url, header=None, sep=r'\\s+')\n",
    "            df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "            X = df.iloc[:, :-1].values\n",
    "            y = df.iloc[:, -1].values - 1  # 0-index\n",
    "            feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading Satimage dataset: {e}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    # Ensure numeric arrays\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    return X, y, feature_names\n",
    "\n",
    "# =======================================================================\n",
    "# DLGN MODEL (kept close to your original, small cleanup)\n",
    "# =======================================================================\n",
    "\n",
    "class DLGN_FC(nn.Module):\n",
    "    def __init__(self, input_dim=None, output_dim=None, num_hidden_nodes=None, \n",
    "                 beta=30, dlgn_mode='dlgn_sf', mode='pwc', num_classes=4):\n",
    "        super(DLGN_FC, self).__init__()\n",
    "        if num_hidden_nodes is None:\n",
    "            num_hidden_nodes = []\n",
    "        self.num_hidden_layers = len(num_hidden_nodes)\n",
    "        self.beta = beta\n",
    "        self.dlgn_mode = dlgn_mode\n",
    "        self.mode = mode\n",
    "        self.num_classes = num_classes\n",
    "        self.num_nodes = [input_dim] + list(num_hidden_nodes) + [num_classes]\n",
    "        self.gating_layers = nn.ModuleList()\n",
    "        self.value_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_hidden_layers+1):\n",
    "            if i != self.num_hidden_layers:\n",
    "                if self.dlgn_mode == 'dlgn_sf':\n",
    "                    temp = nn.Linear(self.num_nodes[0], self.num_nodes[i+1], bias=False)\n",
    "                else:\n",
    "                    temp = nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False)\n",
    "                self.gating_layers.append(temp)\n",
    "            temp = nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False)\n",
    "            self.value_layers.append(temp)\n",
    "    \n",
    "    def return_gating_functions(self):\n",
    "        effective_weights = []\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            curr_weight = self.gating_layers[i].weight.detach().clone()\n",
    "            if self.dlgn_mode == 'dlgn_sf':\n",
    "                effective_weights.append(curr_weight)\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    effective_weights.append(curr_weight)\n",
    "                else:\n",
    "                    effective_weights.append(torch.matmul(curr_weight, effective_weights[-1]))\n",
    "        return effective_weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        # determine device once\n",
    "        device = x.device if x.is_cuda else torch.device('cpu')\n",
    "        gate_scores = [x.to(device)]\n",
    "\n",
    "        if self.mode == 'pwc':\n",
    "            values = [torch.ones_like(x).to(device)]\n",
    "        else:\n",
    "            values = [x.to(device)]\n",
    "\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            if self.dlgn_mode == 'dlgn_sf':\n",
    "                gate_scores.append((x.to(device) @ self.gating_layers[i].weight.T))\n",
    "            else:\n",
    "                gate_scores.append(self.gating_layers[i](gate_scores[-1]))\n",
    "            curr_gate_on_off = torch.sigmoid(self.beta * gate_scores[-1])\n",
    "            values.append(self.value_layers[i](values[-1]) * curr_gate_on_off)\n",
    "        \n",
    "        values.append(self.value_layers[self.num_hidden_layers](values[-1]))\n",
    "        return values, gate_scores\n",
    "\n",
    "# =======================================================================\n",
    "# TRAINING FUNCTION (FULL DATA)\n",
    "# =======================================================================\n",
    "\n",
    "def train_dlgn_full(DLGN_obj, data_full, labels_full, \n",
    "                    num_epoch=1500, parameter_mask=None, seed=42, lr=0.001, \n",
    "                    no_of_batches=10, saved_epochs=None, x_epoch=1000):\n",
    "    if parameter_mask is None:\n",
    "        parameter_mask = {name: torch.ones_like(param) for name, param in DLGN_obj.named_parameters()}\n",
    "    if saved_epochs is None:\n",
    "        saved_epochs = list(range(0, num_epoch, 100)) + [num_epoch-1]\n",
    "\n",
    "    set_torchseed(seed)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    DLGN_obj.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(DLGN_obj.parameters(), lr=lr)\n",
    "\n",
    "    data_torch = torch.Tensor(data_full)\n",
    "    labels_torch = torch.tensor(labels_full, dtype=torch.int64)\n",
    "\n",
    "    batch_size = max(1, len(data_full) // max(1, no_of_batches))\n",
    "    losses = []\n",
    "    DLGN_obj_store = []\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(saved_epochs[-1] + 1), desc=\"Training on Full Data\"):\n",
    "        if epoch in saved_epochs:\n",
    "            DLGN_obj_copy = deepcopy(DLGN_obj)\n",
    "            DLGN_obj_copy.to(torch.device('cpu'))\n",
    "            DLGN_obj_store.append(DLGN_obj_copy)\n",
    "\n",
    "            train_outputs_values, _ = DLGN_obj(torch.Tensor(data_full).to(device))\n",
    "            targets = torch.tensor(labels_full, dtype=torch.int64).to(device)\n",
    "            train_loss = criterion(train_outputs_values[-1], targets)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "\n",
    "            if train_loss.item() < 5e-6 or np.isnan(train_loss.detach().cpu().numpy()):\n",
    "                break\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for batch_start in range(0, len(data_full), batch_size):\n",
    "            if (batch_start + batch_size) > len(data_full):\n",
    "                # make last batch smaller instead of skipping\n",
    "                inputs = data_torch[batch_start:len(data_full)]\n",
    "                targets = labels_torch[batch_start:len(data_full)]\n",
    "            else:\n",
    "                inputs = data_torch[batch_start:batch_start+batch_size]\n",
    "                targets = labels_torch[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            values, _ = DLGN_obj(inputs)\n",
    "            outputs = values[-1]\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            for name, param in DLGN_obj.named_parameters():\n",
    "                parameter_mask[name] = parameter_mask[name].to(device)\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= parameter_mask[name]\n",
    "                    if \"gat\" in name and epoch > x_epoch:\n",
    "                        param.grad *= 0.\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Track loss after epoch\n",
    "        train_outputs_values, _ = DLGN_obj(torch.Tensor(data_full).to(device))\n",
    "        targets = torch.tensor(labels_full, dtype=torch.int64).to(device)\n",
    "        train_loss = criterion(train_outputs_values[-1], targets)\n",
    "        losses.append(train_loss.cpu().detach().clone().numpy())\n",
    "\n",
    "    DLGN_obj.to(torch.device('cpu'))\n",
    "    return train_losses, DLGN_obj, DLGN_obj_store, losses\n",
    "\n",
    "# =======================================================================\n",
    "# BASELINE MODELS (FULL DATA)\n",
    "# =======================================================================\n",
    "\n",
    "def train_baselines_full(X_full, y_full):\n",
    "    results = {}\n",
    "    print(\"  Training Random Forest on full data...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=20)\n",
    "    rf.fit(X_full, y_full)\n",
    "    rf_pred = rf.predict(X_full)\n",
    "    results['Random Forest'] = accuracy_score(y_full, rf_pred)\n",
    "\n",
    "    print(\"  Training SVM on full data (may be slow for large datasets)...\")\n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_full, y_full)\n",
    "    results['SVM'] = accuracy_score(y_full, svm.predict(X_full))\n",
    "\n",
    "    print(\"  Training CART on full data...\")\n",
    "    cart = DecisionTreeClassifier(random_state=42, max_depth=20)\n",
    "    cart.fit(X_full, y_full)\n",
    "    results['CART'] = accuracy_score(y_full, cart.predict(X_full))\n",
    "\n",
    "    return results\n",
    "\n",
    "# =======================================================================\n",
    "# MAIN EXPERIMENT RUNNER (FULL DATASET)\n",
    "# =======================================================================\n",
    "\n",
    "def run_experiment_full(dataset_name, config, sample_frac=None):\n",
    "    \"\"\"\n",
    "    dataset_name: str\n",
    "    config: config dict for DLGN training\n",
    "    sample_frac: override the default sampling fraction (None uses loader default)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EXPERIMENT: {dataset_name.upper()} Dataset (FULL DATA)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    try:\n",
    "        if sample_frac is None:\n",
    "            X, y, feature_names = load_dataset(dataset_name)\n",
    "        else:\n",
    "            X, y, feature_names = load_dataset(dataset_name, sample_frac=sample_frac)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not load dataset {dataset_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    num_classes = len(np.unique(y))\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    print(f\"   Total Samples: {X.shape[0]}\")\n",
    "    print(f\"   Features: {input_dim}\")\n",
    "    print(f\"   Classes: {num_classes}\")\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    print(f\"   Using ALL {len(X_scaled)} samples for training AND testing\")\n",
    "\n",
    "    # Train baselines\n",
    "    print(\"\\n2. Training Baseline Models on Full Data...\")\n",
    "    baseline_results = train_baselines_full(X_scaled, y)\n",
    "\n",
    "    # Train DLGN\n",
    "    print(\"\\n3. Training DLGN on Full Data...\")\n",
    "\n",
    "    set_torchseed(config.get('seed', 6675))\n",
    "    DLGN_model = DLGN_FC(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=1,\n",
    "        num_hidden_nodes=config['hidden_nodes'],\n",
    "        beta=config['beta'],\n",
    "        dlgn_mode=config.get('dlgn_mode', 'dlgn'),\n",
    "        mode='pwc',\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    train_parameter_masks = {name: torch.ones_like(param) for name, param in DLGN_model.named_parameters()}\n",
    "\n",
    "    train_losses, DLGN_final, DLGN_store, losses = train_dlgn_full(\n",
    "        DLGN_obj=deepcopy(DLGN_model),\n",
    "        data_full=X_scaled,\n",
    "        labels_full=y,\n",
    "        parameter_mask=train_parameter_masks,\n",
    "        lr=config['lr'],\n",
    "        no_of_batches=config['batches'],\n",
    "        saved_epochs=config['saved_epochs'],\n",
    "        x_epoch=config['x_epoch'],\n",
    "        num_epoch=config['num_epochs'],\n",
    "        seed=config.get('seed', 5000)\n",
    "    )\n",
    "\n",
    "    # Evaluate DLGN on full data\n",
    "    full_outputs_values, _ = DLGN_final(torch.Tensor(X_scaled))\n",
    "    full_logits = full_outputs_values[-1].detach().numpy()\n",
    "    full_preds = np.argmax(full_logits, axis=1)\n",
    "    dlgn_acc = accuracy_score(y, full_preds)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS (Training and Testing on FULL DATASET)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Model':<20} {'Accuracy on Full Data':<25}\")\n",
    "    print(\"-\"*45)\n",
    "    print(f\"{'DLGN':<20} {dlgn_acc:.4f}\")\n",
    "    for model_name, acc in baseline_results.items():\n",
    "        print(f\"{model_name:<20} {acc:.4f}\")\n",
    "\n",
    "    # Per-class accuracy for DLGN\n",
    "    print(f\"\\nDLGN Per-Class Accuracy (on full data):\")\n",
    "    for i in range(num_classes):\n",
    "        class_mask = y == i\n",
    "        class_correct = np.sum((y[class_mask] == full_preds[class_mask]))\n",
    "        class_total = np.sum(class_mask)\n",
    "        if class_total > 0:\n",
    "            print(f\"  Class {i}: {class_correct / class_total:.4f} ({class_correct}/{class_total})\")\n",
    "\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'dlgn_acc': dlgn_acc,\n",
    "        'baseline_results': baseline_results,\n",
    "        'num_samples': X.shape[0],\n",
    "        'num_features': input_dim,\n",
    "        'num_classes': num_classes\n",
    "    }\n",
    "\n",
    "# =======================================================================\n",
    "# RUNNING MULTIPLE EXPERIMENTS\n",
    "# =======================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # DLGN configs tuned by dataset scale\n",
    "    small_dataset_config = {\n",
    "        'hidden_nodes': [10, 10, 10],\n",
    "        'beta': 4,\n",
    "        'lr': 0.002,\n",
    "        'batches': 5,\n",
    "        'dlgn_mode': 'dlgn',\n",
    "        'num_epochs': 500,\n",
    "        'saved_epochs': list(range(0, 501, 100)),\n",
    "        'x_epoch': 300,\n",
    "        'seed': 6675\n",
    "    }\n",
    "\n",
    "    medium_dataset_config = {\n",
    "        'hidden_nodes': [20, 20, 20],\n",
    "        'beta': 4,\n",
    "        'lr': 0.002,\n",
    "        'batches': 10,\n",
    "        'dlgn_mode': 'dlgn',\n",
    "        'num_epochs': 800,\n",
    "        'saved_epochs': list(range(0, 801, 100)),\n",
    "        'x_epoch': 500,\n",
    "        'seed': 6675\n",
    "    }\n",
    "\n",
    "    large_dataset_config = {\n",
    "        'hidden_nodes': [64, 64, 32],\n",
    "        'beta': 4,\n",
    "        'lr': 0.001,\n",
    "        'batches': 50,\n",
    "        'dlgn_mode': 'dlgn',\n",
    "        'num_epochs': 400,\n",
    "        'saved_epochs': list(range(0, 401, 50)),\n",
    "        'x_epoch': 300,\n",
    "        'seed': 6675\n",
    "    }\n",
    "\n",
    "    # Datasets to run (the 7 requested)\n",
    "    datasets_and_configs = [\n",
    "        ('covertype', large_dataset_config, 0.02),  # sample 2% by default to speed up testing\n",
    "        ('poker', large_dataset_config, 0.01),      # sample 1% (1M rows -> ~10k rows)\n",
    "        ('higgs', large_dataset_config, 0.001),     # sample 0.1% by default (11M -> ~11k rows)\n",
    "        ('letter', medium_dataset_config, 1.0),\n",
    "        ('pendigits', medium_dataset_config, 1.0),\n",
    "        ('gas', medium_dataset_config, 1.0),\n",
    "        ('satimage', medium_dataset_config, 1.0)\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    for name, cfg, sample_frac in datasets_and_configs:\n",
    "        try:\n",
    "            res = run_experiment_full(name, cfg, sample_frac=sample_frac)\n",
    "            if res:\n",
    "                all_results.append(res)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Experiment {name} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Summary\n",
    "    if all_results:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY OF ALL EXPERIMENTS (FULL DATASET RESULTS)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\n{'Dataset':<15} {'Samples':<10} {'Features':<10} {'Classes':<10} {'DLGN':<10} {'RF':<10} {'SVM':<10} {'CART':<10}\")\n",
    "        print(\"-\"*95)\n",
    "        for r in all_results:\n",
    "            ds = r['dataset']\n",
    "            print(f\"{ds:<15} {r['num_samples']:<10} {r['num_features']:<10} {r['num_classes']:<10} \"\n",
    "                  f\"{r['dlgn_acc']:.4f}     {r['baseline_results']['Random Forest']:.4f}     \"\n",
    "                  f\"{r['baseline_results']['SVM']:.4f}     {r['baseline_results']['CART']:.4f}\")\n",
    "        print(\"\\nALL EXPERIMENTS COMPLETE!\")\n",
    "    else:\n",
    "        print(\"No successful experiments completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090103e4",
   "metadata": {
    "papermill": {
     "duration": 0.063762,
     "end_time": "2025-11-19T06:17:24.632110",
     "exception": false,
     "start_time": "2025-11-19T06:17:24.568348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 584.425325,
   "end_time": "2025-11-19T06:17:27.338814",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T06:07:42.913489",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
